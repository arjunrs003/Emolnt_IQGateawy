# -*- coding: utf-8 -*-
"""Emolnt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAYD1OoDN2_dfZcIgbN33wzTr5RVMJYL
"""

import pandas as pd
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
import string
from nltk.tokenize import word_tokenize
import re
from nltk.stem import WordNetLemmatizer
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np
import scipy



path = "/content/drive/MyDrive/"
cols = ['id', 'text', 'label', 'intensity']
anger = pd.read_csv(path + 'train_anger.txt', header=None, sep='\t', names=cols, index_col=0)
fear = pd.read_csv(path + 'train_fear.txt', header=None, sep='\t', names=cols, index_col=0)
sad = pd.read_csv(path + 'train_sadness.txt', header=None, sep='\t', names=cols, index_col=0)
joy = pd.read_csv(path + 'train_joy.txt', header=None, sep='\t', names=cols, index_col=0)

express = [anger, fear, sad, joy]
df_train = pd.concat(express)
df_train.reset_index(inplace=True)
df_train.label.value_counts()


punctuations = string.punctuation
stopwords = stopwords.words("english")
total_stopwords = stopwords+list(punctuations)
total_stopwords.remove("not")

def contractions(s):
    s = re.sub(r"won't", "will not",s)
    s = re.sub(r"would't", "would not",s)
    s = re.sub(r"could't", "could not",s)
    s = re.sub(r"\'d", "would",s)
    s = re.sub(r"can\'t", "can not",s)
    s = re.sub(r"n\'t", "not", s)
    s= re.sub(r"\'re", "are", s)
    s = re.sub(r"\'s", "is", s)
    s = re.sub(r"\"ll", "will", s)
    s = re.sub(r"\'t", "not", s)
    s = re.sub(r"\'ve", "have", s)
    s = re.sub(r"\'m", "am", s)
    s = re.sub("([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)"," ",s)
    s = re.sub("[^A-Za-z ]"," ",s)
    return s
df_train['text']=df_train['text'].apply(lambda x:contractions(x))

lema = WordNetLemmatizer()
def clean_data(text):
    cleaned = []
    for headline in text:
        headlines = re.sub(r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"," ",headline)   #url
        headlines = re.sub("([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)"," ",headlines)       #email
        headlines = re.sub("[^A-Za-z ]"," ",headlines)
        headlines = headlines.lower()
        headlines = word_tokenize(headlines)
        # print(headlines)
        headlines = [lema.lemmatize(word) for word in headlines if word not in total_stopwords]
        cleaned.append(" ".join(headlines))
    return cleaned

text = clean_data(df_train['text'])

df_train.text = pd.DataFrame(text)

label_encoder = LabelEncoder()

df_train['label']= label_encoder.fit_transform(df_train['label'])

x_df_train = df_train.drop('intensity',axis=1)

y_df_train = df_train['intensity']



vectorizer_tfidf = TfidfVectorizer(max_features=1000)
x_tfidf = vectorizer_tfidf.fit_transform(x_df_train['text'])
x_tfidf_df = pd.DataFrame.sparse.from_spmatrix(x_tfidf).join(x_df_train['label'])


x_tfidf_df.columns = x_tfidf_df.columns.astype(str)

np_df=x_tfidf_df.to_numpy()

y_tfidf=df_train['intensity']

NB_CLASSES=3
model=tf.keras.models.Sequential()
model.add(keras.layers.Dense(128,
                           input_shape=(1001,),

                           activation='relu'))

model.add(keras.layers.Dense(128,
                           activation='relu'))


model.add(keras.layers.Dense(1,

                           activation='relu'))


model.compile(loss='mean_squared_error', optimizer='Adam')
model.summary()

VERBOSE=1
BATCH_SIZE=16
EPOCHS=10
VALIDATION_SPLIT=0.2

#fit model
history=model.fit(np_df,y_tfidf,batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
pd.DataFrame(history.history).plot(figsize=(8,5))
plt.title('Accuracy improvements with epochs')
plt.show()

# model.evaluate(x_test,y_test)

path = "/content/drive/MyDrive/"
cols = ['id', 'text', 'label', 'intensity']
dev_anger = pd.read_csv(path + 'dev_anger.txt', header=None, sep='\t', names=cols, index_col=0)
dev_fear = pd.read_csv(path + 'dev_fear.txt', header=None, sep='\t', names=cols, index_col=0)
dev_sad = pd.read_csv(path + 'dev_sadness.txt', header=None, sep='\t', names=cols, index_col=0)
dev_joy = pd.read_csv(path + 'dev_joy.txt', header=None, sep='\t', names=cols, index_col=0)
dev_anger_intensity = pd.read_csv(path + 'dev_anger_intensity.txt', header=None, sep='\t', names=cols, index_col=0)
dev_fear_intensity = pd.read_csv(path + 'dev_fear_intensity.txt', header=None, sep='\t', names=cols, index_col=0)
dev_sad_intensity = pd.read_csv(path + 'dev_sadness_intensity.txt', header=None, sep='\t', names=cols, index_col=0)
dev_joy_intensity = pd.read_csv(path + 'dev_joy_intensity.txt', header=None, sep='\t', names=cols, index_col=0)

express = [dev_anger, dev_fear, dev_sad, dev_joy,dev_anger_intensity, dev_fear_intensity, dev_sad_intensity, dev_joy_intensity]
dev_df = pd.concat(express)
dev_df.reset_index(inplace=True)
dev_df.label.value_counts()

dev_df['text']=dev_df['text'].apply(lambda x:contractions(x))

text = clean_data(dev_df['text'])

dev_df.text = pd.DataFrame(text)

dev_df['label']= label_encoder.fit_transform(dev_df['label'])

x_dev_df = dev_df.drop('intensity',axis=1)
x_dev_df.shape

y_dev_df = dev_df['intensity']


y_dev_df=pd.to_numeric(y_dev_df, errors='coerce').fillna(0)


vectorizer_tfidf = TfidfVectorizer(max_features=1000)
x_dev_df_tfidf = vectorizer_tfidf.fit_transform(x_dev_df['text'])
x_dev_df_tfidf_df = pd.DataFrame.sparse.from_spmatrix(x_dev_df_tfidf).join(x_dev_df['label'])

x_dev_df_tfidf_df.columns = x_dev_df_tfidf_df.columns.astype(str)
x_dev_df_tfidf_np = x_dev_df_tfidf_df.to_numpy()



def get_score(y_test, y_pred, name):
    print(name + '\n')
    print('Mean Absolute Error:', mean_absolute_error(y_test, y_pred))
    print('Mean Squared Error:', mean_squared_error(y_test, y_pred))
    print('Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))
    return mean_absolute_error(y_test, y_pred),mean_squared_error(y_test, y_pred),np.sqrt(mean_squared_error(y_test, y_pred))

model.predict(x_dev_df_tfidf_np)

get_score(y_dev_df,model.predict(x_dev_df_tfidf_np),'ANN')

y_actual = y_dev_df
y_predicted = model.predict(x_dev_df_tfidf_np).flatten()
print(y_predicted)

pd.DataFrame(data={"Actual Intensity" : y_actual, "Predicted Intensity" : y_predicted})

combined_df = pd.concat([df_train, dev_df]).reset_index()
combined_df.shape

combined_df.isna().sum()

combined_df['intensity']=pd.to_numeric(combined_df['intensity'], errors='coerce').fillna(0)

anger = combined_df.loc[combined_df['label']==0]
x_anger = anger.drop('intensity',axis=1)
anger['label']=pd.to_numeric(anger['label'], errors='coerce').fillna(0)

anger_vectorizer = TfidfVectorizer(max_features=1000)
x_anger_tfidf = anger_vectorizer.fit_transform(x_anger['text'])
x_anger_tdidf_df = pd.DataFrame.sparse.from_spmatrix(x_anger_tfidf).join(anger['label'])
x_anger_tdidf_df.columns = x_anger_tdidf_df.columns.astype(str)
y_anger_intensity = anger['intensity']
np_df_anger=x_anger_tdidf_df.to_numpy()

#Anger_model
anger_model=tf.keras.models.Sequential()
anger_model.add(keras.layers.Dense(128,
                          input_shape=(1001,),
                          activation='relu'))
anger_model.add(keras.layers.Dense(128,
                          activation='relu'))
anger_model.add(keras.layers.Dense(1,
                          activation='relu'))
anger_model.compile(loss='mean_squared_error', optimizer='Adam')
anger_model.summary()

fear_model=tf.keras.models.Sequential()
fear_model.add(keras.layers.Dense(128,
                          input_shape=(1001,),

                          activation='relu'))
fear_model.add(keras.layers.Dense(128,
                          activation='relu'))
fear_model.add(keras.layers.Dense(1,
                          #name='outlput_layer',
                          activation='relu'))
fear_model.compile(loss='mean_squared_error', optimizer='Adam')
fear_model.summary()

sad_model=tf.keras.models.Sequential()
sad_model.add(keras.layers.Dense(128,
                          input_shape=(1001,),

                          activation='relu'))
sad_model.add(keras.layers.Dense(128,
                          activation='relu'))
sad_model.add(keras.layers.Dense(1,
                          #name='outlput_layer',
                          activation='relu'))
sad_model.compile(loss='mean_squared_error', optimizer='Adam')
sad_model.summary()

joy_model=tf.keras.models.Sequential()
joy_model.add(keras.layers.Dense(128,
                          input_shape=(1001,),

                          activation='relu'))
joy_model.add(keras.layers.Dense(128,
                          activation='relu'))
joy_model.add(keras.layers.Dense(1,
                          #name='outlput_layer',
                          activation='relu'))
joy_model.compile(loss='mean_squared_error', optimizer='Adam')
joy_model.summary()

anger = combined_df.loc[combined_df['label']==0]
anger_vectorizer = TfidfVectorizer(max_features=1000)
x_anger = anger_vectorizer.fit_transform(anger['text'])
x_anger_df = pd.DataFrame.sparse.from_spmatrix(x_anger).join(anger['label'])
y_intensity = anger['intensity']
np_df_anger=x_anger_df.to_numpy()
anger_model_history = anger_model.fit(np_df_anger, y_intensity)

vectorizer_tfidf = TfidfVectorizer(max_features=1000)
x_dev_df_tfidf = vectorizer_tfidf.fit_transform(x_dev_df['text'])
x_dev_df_tfidf_df = pd.DataFrame.sparse.from_spmatrix(x_dev_df_tfidf).join(x_dev_df['label'])

anger = combined_df.loc[combined_df['label']==0]
anger_vectorizer = TfidfVectorizer(max_features=1000)
x_anger = anger_vectorizer.fit_transform(anger['text'])
x_anger_df = pd.DataFrame.sparse.from_spmatrix(x_anger).join(anger['label'])
np_df_anger=x_anger_df.to_numpy()
y_intensity = anger['intensity']
anger_model_history = anger_model.fit(np_df_anger, y_intensity)

#FEAR
fear = combined_df.loc[combined_df['label']==1]
fear_vectorizer = TfidfVectorizer(max_features=1000)
x_fear = fear_vectorizer.fit_transform(fear['text'])
x_fear_df = pd.DataFrame.sparse.from_spmatrix(x_fear).join(fear['label'])
np_df_fear=x_fear_df.to_numpy()
y_intensity_fear=fear['intensity']
fear_model_history = fear_model.fit(np_df_fear, y_intensity_fear)

#SADNESS
sad = combined_df.loc[combined_df['label']==3]
sad_vectorizer = TfidfVectorizer(max_features=1000)
x_sad = sad_vectorizer.fit_transform(sad['text'])
x_sad_df = pd.DataFrame.sparse.from_spmatrix(x_sad).join(sad['label'])
np_df_sad=x_sad_df.to_numpy()
y_intensity_sad=sad['intensity']
sad_model_history = sad_model.fit(np_df_sad, y_intensity_sad)

#JOY
joy = combined_df.loc[combined_df['label']==2]
joy_vectorizer = TfidfVectorizer(max_features=1000)
x_joy = joy_vectorizer.fit_transform(joy['text'])
x_joy_df = pd.DataFrame.sparse.from_spmatrix(x_joy).join(joy['label'])
np_df_joy=x_joy_df.to_numpy()
y_intensity_joy=joy['intensity']
joy_model_history = joy_model.fit(np_df_joy, y_intensity_joy)

cols = ["id", "text", "label", "intensity"]
anger_test = pd.read_csv(path + "test_anger.txt", header=None, sep="\t", names=cols, index_col=0)
fear_test = pd.read_csv(path + "test_fear.txt", header=None, sep="\t", names=cols, index_col=0)
sad_test = pd.read_csv(path + "test_sadness.txt", header=None, sep="\t", names=cols, index_col=0)
joy_test = pd.read_csv(path + "test_joy.txt", header=None, sep="\t", names=cols, index_col=0)
anger_test_intensity = pd.read_csv(path + "test_anger_intensity.txt", header=None, sep="\t", names=cols, index_col=0)
fear_test_intensity = pd.read_csv(path + "test_fear_intensity.txt", header=None, sep="\t", names=cols, index_col=0)
sad_test_intensity = pd.read_csv(path + "test_sadness_intensity.txt", header=None, sep="\t", names=cols, index_col=0)
joy_test_intensity = pd.read_csv(path + "test_joy_intensity.txt", header=None, sep="\t", names=cols, index_col=0)


w_intensity = [anger_test,fear_test,sad_test,joy_test,anger_test_intensity,fear_test_intensity,sad_test_intensity,joy_test_intensity]
for i in w_intensity:
  i['intensity']=pd.to_numeric(i['intensity'], errors='coerce').fillna(0)
  text = clean_data(i['text'])
  i['text']=i['text'].apply(lambda x:contractions(x))

express = [anger_test,anger_test_intensity]
df_test_anger = pd.concat(express)
df_test_anger.reset_index(inplace=True)
df_test_anger.label.value_counts()

df_test_anger['label'] = label_encoder.fit_transform(df_test_anger['label'])

express = [fear_test,fear_test_intensity]
df_test_fear = pd.concat(express)
df_test_fear.reset_index(inplace=True)
df_test_fear.label.value_counts()
df_test_fear['label'] = label_encoder.fit_transform(df_test_fear['label'])
df_test_fear['intensity'] = pd.to_numeric(df_test_fear['intensity'], errors='coerce').fillna(0)

express = [sad_test,sad_test_intensity]
df_test_sad = pd.concat(express)
df_test_sad.reset_index(inplace=True)
df_test_sad.label.value_counts()
df_test_sad['label'] = label_encoder.fit_transform(df_test_sad['label'])
df_test_sad['intensity'] = pd.to_numeric(df_test_sad['intensity'], errors='coerce').fillna(0)

express = [joy_test,joy_test_intensity]
df_test_joy = pd.concat(express)
df_test_joy.reset_index(inplace=True)
df_test_joy.label.value_counts()
df_test_joy['label'] = label_encoder.fit_transform(df_test_joy['label'])
df_test_joy['intensity'] = pd.to_numeric(df_test_joy['intensity'], errors='coerce').fillna(0)

df_test_anger['intensity'] = pd.to_numeric(df_test_anger['intensity'], errors='coerce').fillna(0)

# X_anger_test = df_test_anger.drop(['intensity'],axis=1)
# Y_anger_actual = df_test_anger['intensity']

#ANGER
X_anger_test = anger_vectorizer.transform(df_test_anger['text'])
x_anger_test_df = pd.DataFrame.sparse.from_spmatrix(X_anger_test).join(df_test_anger['label'])
np_anger_test=x_anger_test_df.to_numpy()
Y_anger_actual=tf.keras.utils.to_categorical(df_test_anger['intensity'],4)
Y_anger_predicted = anger_model.predict(x_anger_test_df)

#FEAR
X_fear_test = fear_vectorizer.transform(df_test_fear['text'])
x_fear_test_df = pd.DataFrame.sparse.from_spmatrix(X_fear_test).join(df_test_fear['label'])
np_fear_test=x_fear_test_df.to_numpy()
Y_fear_actual=tf.keras.utils.to_categorical(df_test_fear['intensity'],4)
Y_fear_predicted = fear_model.predict(np_fear_test)

#SADNESS
X_sad_test = sad_vectorizer.transform(df_test_sad['text'])
x_sad_test_df = pd.DataFrame.sparse.from_spmatrix(X_sad_test).join(df_test_sad['label'])
np_sad_test=x_sad_test_df.to_numpy()
Y_sad_actual=tf.keras.utils.to_categorical(df_test_sad['intensity'],4)
Y_sad_predicted = sad_model.predict(np_sad_test)

#JOY
X_joy_test = joy_vectorizer.transform(df_test_joy['text'])
x_joy_test_df = pd.DataFrame.sparse.from_spmatrix(X_joy_test).join(df_test_joy['label'])
np_joy_test=x_joy_test_df.to_numpy()
Y_joy_actual=tf.keras.utils.to_categorical(df_test_joy['intensity'],4)
Y_joy_predicted = joy_model.predict(np_joy_test)

get_score(Y_anger_actual, Y_anger_predicted, "Result of Anger Model")
get_score(Y_fear_actual, Y_fear_predicted, "Result of Fear Model")
get_score(Y_sad_actual, Y_sad_predicted, "Result of Sadness Model")
get_score(Y_joy_actual, Y_joy_predicted, "Result of Joy Model")


def evaluate(pred,gold):
    gold_scores=[]
    pred_scores=[]
    gold_scores_range_05_1=[]
    pred_scores_range_05_1=[]
    for p in pred:
        pred_scores.append(p)
    for g in gold:
        gold_scores.append(g)
    for i in range(len(gold_scores)):
        if gold_scores[i] >= 0.5:
            gold_scores_range_05_1.append(gold_scores[i])
            pred_scores_range_05_1.append(pred_scores[i])

    if np.std(pred_scores)==0 or np.std(gold_scores)==0:
        return (0,0,0,0)

    pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]
    pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]

    return (pears_corr,pears_corr_range_05_1)

pear_results=[]
spear_results=[]
pear_results_range_05_1=[]
spear_results_range_05_1=[]

num_pairs = 4
argv = ["Anger_Actual", Y_anger_actual, "Anger_Predicted", Y_anger_predicted, "Fear_Actual", Y_fear_actual, "Fear_Predicted", Y_fear_predicted, "Sad_Actual", Y_sad_actual, "Sad_Predicted", Y_sad_predicted, "Joy_Actual", Y_joy_actual, "Joy_Predicted", Y_joy_predicted]

for i in range(0,len(argv),4):
    name_gold = argv[i]
    gold=argv[i+1]
    name_pred = argv[i+2]
    pred=argv[i+3]
    result=evaluate(pred,gold)
    print ("Pearson correlation between ", name_pred, " and ", name_gold, ":\t", str(result[0]))
    pear_results.append(result[0])
    print ("Pearson correlation for gold scores in range 0.5-1 between ",name_pred," and ",name_gold,":\t",str(result[1]))
    pear_results_range_05_1.append(result[1])

avg_pear=np.mean(pear_results)

avg_pear_range_05_1=np.mean(pear_results_range_05_1)

print ("Average Pearson correlation:\t",str(avg_pear))

print ("Average Pearson correlation for gold scores in range 0.5-1:\t", str(avg_pear_range_05_1))

